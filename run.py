# run.py
import os
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

def run_with_proxy():
    """ä½¿ç”¨ä»£ç†é‹è¡Œçˆ¬èŸ²"""
    try:
        # å˜—è©¦åŒ¯å…¥ä»£ç†ç›¸é—œæ¨¡çµ„
        from twstock.proxy import get_proxies, check_proxy, ProxyPool
        from twstock.logger import init_log_file, append_log
        
        print("ğŸ” é–‹å§‹æª¢æŸ¥ä»£ç†...")
        
        # åˆå§‹åŒ–æ—¥èªŒ
        log_path = init_log_file('å€‹è‚¡æ—¥æˆäº¤è³‡è¨Š/logs')
        
        # 1. æŠ“å–ä¸¦ç¯©é¸å‡ºæœ‰æ•ˆä»£ç†
        all_proxies = get_proxies()
        print(f"ğŸ“‹ å–å¾— {len(all_proxies)} å€‹ä»£ç†")
        
        valid_proxies = []
        for proxy in all_proxies:
            print(f"ğŸ” æª¢æŸ¥ä»£ç†: {proxy}")
            if check_proxy(proxy):
                valid_proxies.append(proxy)
                print(f"âœ… ä»£ç†æœ‰æ•ˆ: {proxy}")
        
        if not valid_proxies:
            print("âŒ æ²’æœ‰ä»»ä½•æœ‰æ•ˆä»£ç†ï¼æ”¹ç”¨ç„¡ä»£ç†æ¨¡å¼...")
            return run_without_proxy()
        
        print(f"âœ… æ‰¾åˆ° {len(valid_proxies)} å€‹æœ‰æ•ˆä»£ç†")
        append_log(log_path, 'ä»£ç†ç®¡ç†', 'ç³»çµ±', f'æœ‰æ•ˆä»£ç†æ•¸é‡: {len(valid_proxies)}')
        
        # 2. å»ºç«‹ä»£ç†æ± å¯¦ä¾‹
        proxy_pool = ProxyPool(valid_proxies)
        
        # 3. ç²å–ä¸¦ä¿®æ”¹è¨­å®š
        settings = get_project_settings()
        settings.set('PROXY_POOL', valid_proxies)
        settings.set('PROXY_POOL_INSTANCE', proxy_pool)
        
        # 4. å•Ÿç”¨ä»£ç†ä¸­ä»‹è»Ÿé«”
        middlewares = settings.get('DOWNLOADER_MIDDLEWARES', {})
        middlewares['twstock.middlewares.ProxyMiddleware'] = 100
        settings.set('DOWNLOADER_MIDDLEWARES', middlewares)
        
        print("ğŸš€ ä½¿ç”¨ä»£ç†æ¨¡å¼å•Ÿå‹•çˆ¬èŸ²...")
        
        # 5. å•Ÿå‹•çˆ¬èŸ²
        process = CrawlerProcess(settings)
        process.crawl('daily')  # ä½¿ç”¨ä½ çš„çˆ¬èŸ²åç¨±
        process.start()
        
    except ImportError as e:
        print(f"âš ï¸ ç„¡æ³•åŒ¯å…¥ä»£ç†æ¨¡çµ„: {e}")
        print("ğŸ’¡ æ”¹ç”¨ç„¡ä»£ç†æ¨¡å¼...")
        return run_without_proxy()
    except Exception as e:
        print(f"âŒ ä»£ç†è¨­å®šå¤±æ•—: {e}")
        print("ğŸ’¡ æ”¹ç”¨ç„¡ä»£ç†æ¨¡å¼...")
        return run_without_proxy()

def run_without_proxy():
    """ä¸ä½¿ç”¨ä»£ç†é‹è¡Œçˆ¬èŸ²"""
    print("ğŸš€ ç„¡ä»£ç†æ¨¡å¼å•Ÿå‹•çˆ¬èŸ²...")
    
    try:
        # ç²å–å°ˆæ¡ˆè¨­å®š
        settings = get_project_settings()
        
        # ç¢ºä¿ä»£ç†ä¸­ä»‹è»Ÿé«”è¢«åœç”¨
        middlewares = settings.get('DOWNLOADER_MIDDLEWARES', {})
        if 'twstock.middlewares.ProxyMiddleware' in middlewares:
            del middlewares['twstock.middlewares.ProxyMiddleware']
            settings.set('DOWNLOADER_MIDDLEWARES', middlewares)
        
        # å•Ÿå‹•çˆ¬èŸ²
        process = CrawlerProcess(settings)
        process.crawl('daily')
        process.start()
        
    except Exception as e:
        print(f"âŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 50)
    print("ğŸ•·ï¸ Taiwan Stock Daily Data Crawler")
    print("=" * 50)
    
    # æª¢æŸ¥ç›®éŒ„
    if not os.path.exists('scrapy.cfg'):
        print("âŒ æ‰¾ä¸åˆ° scrapy.cfgï¼Œè«‹ç¢ºèªåœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„åŸ·è¡Œ")
        return
    
    # è©¢å•é‹è¡Œæ¨¡å¼
    print("\nè«‹é¸æ“‡é‹è¡Œæ¨¡å¼:")
    print("1. ç„¡ä»£ç†æ¨¡å¼ (æ¨è–¦)")
    print("2. ä»£ç†æ¨¡å¼")
    
    try:
        choice = input("è«‹è¼¸å…¥é¸æ“‡ (1 æˆ– 2ï¼Œé è¨­ç‚º 1): ").strip()
        
        if choice == "2":
            run_with_proxy()
        else:
            run_without_proxy()
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ¶å–æ¶ˆé‹è¡Œ")
    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")

if __name__ == "__main__":
    main()
